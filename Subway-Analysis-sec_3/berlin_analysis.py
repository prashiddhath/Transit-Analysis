# -*- coding: utf-8 -*-
"""Berlin Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17bAlaS19NPZDOc00mCSktM178KcdloYN
"""

from google.colab import drive

drive.mount('/content/drive')

import os
import pandas as pd
import matplotlib.pyplot as plt
import folium

base_path = "/content/drive/MyDrive/berlin_ubahn"

# Load Berlin U-Bahn subset
u_routes = pd.read_csv(os.path.join(base_path, "u_routes.csv"))
u_trips = pd.read_csv(os.path.join(base_path, "u_trips.csv"))
u_stop_times = pd.read_csv(os.path.join(base_path, "u_stop_times.csv"))
u_stops = pd.read_csv(os.path.join(base_path, "u_stops.csv"))
st_times_with_coords = pd.read_csv(os.path.join(base_path, "u_stop_times_with_coords.csv"))
rep_trips = pd.read_csv(os.path.join(base_path, "u_rep_trips.csv"))
rep_st_times = pd.read_csv(os.path.join(base_path, "u_rep_stop_times_with_coords.csv"))

print("Loaded:")
print("  routes:", u_routes.shape)
print("  trips:", u_trips.shape)
print("  stop_times:", u_stop_times.shape)
print("  stops:", u_stops.shape)
print("  stop_times_with_coords:", st_times_with_coords.shape)
print("  rep_trips:", rep_trips.shape)
print("  rep_stop_times_with_coords:", rep_st_times.shape)

# Unique U-Bahn lines
print("U-Bahn lines:", sorted(u_routes["route_short_name"].unique()))

# Sample of stops
u_stops[["stop_id", "stop_name", "stop_lat", "stop_lon"]].head()

# Simple static plot of all U-Bahn stops
plt.figure(figsize=(6, 6))
plt.scatter(u_stops["stop_lon"], u_stops["stop_lat"], s=8, alpha=0.8)
plt.gca().set_aspect("equal", adjustable="box")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Berlin U-Bahn stops")
plt.show()

# Center map on mean of U-Bahn stops
center_lat = u_stops["stop_lat"].mean()
center_lon = u_stops["stop_lon"].mean()

m = folium.Map(location=[center_lat, center_lon], zoom_start=12)

# One representative trip per route (from rep_trips / rep_st_times)
for _, trip_row in rep_trips.iterrows():
    trip_id = trip_row["trip_id"]
    route_short = trip_row.get("route_short_name_str", trip_row.get("route_short_name", ""))

    g = rep_st_times[rep_st_times["trip_id"] == trip_id].sort_values("stop_sequence")
    coords = list(zip(g["stop_lat"], g["stop_lon"]))
    if len(coords) < 2:
        continue

    folium.PolyLine(
        locations=coords,
        weight=4,
        opacity=0.8,
        tooltip=f"Line {route_short}"
    ).add_to(m)

# Add stops as small circles
for _, row in u_stops.iterrows():
    folium.CircleMarker(
        location=[row["stop_lat"], row["stop_lon"]],
        radius=3,
        popup=row.get("stop_name", row["stop_id"]),
        fill=True,
        fill_opacity=0.9,
    ).add_to(m)

m  # in Colab, just put the map object as last line to display it

import networkx as nx

G = nx.Graph()

# Add nodes with attributes
for _, row in u_stops.iterrows():
    G.add_node(
        row["stop_id"],
        name=row.get("stop_name", ""),
        lat=row["stop_lat"],
        lon=row["stop_lon"]
    )

# Sort stop_times to ensure proper order
st = u_stop_times.sort_values(["trip_id", "stop_sequence"])

# Add edges between consecutive stops in each trip
for trip_id, group in st.groupby("trip_id"):
    seq = list(group["stop_id"])
    for a, b in zip(seq[:-1], seq[1:]):
        if a != b:
            G.add_edge(a, b)

print("Graph built:")
print("  #nodes:", G.number_of_nodes())
print("  #edges:", G.number_of_edges())

# --- Basic network stats ---
print("Is connected?:", nx.is_connected(G))
giant_comp = max(nx.connected_components(G), key=len)
G_gc = G.subgraph(giant_comp).copy()
print("Nodes in giant component:", G_gc.number_of_nodes())

avg_sp = nx.average_shortest_path_length(G_gc)
diameter = nx.diameter(G_gc)

print("Average shortest path length (giant comp):", avg_sp)
print("Diameter (giant comp):", diameter)

# --- Degree distribution ---
degrees = [deg for _, deg in G_gc.degree()]

plt.figure(figsize=(6, 4))
plt.hist(degrees, bins=range(1, max(degrees) + 2), align="left")
plt.xlabel("Degree (number of neighbors)")
plt.ylabel("Count of stations")
plt.title("Degree distribution of Berlin U-Bahn stations")
plt.tight_layout()
plt.show()

# --- Centrality measures on giant component ---
bet = nx.betweenness_centrality(G_gc)
close = nx.closeness_centrality(G_gc)
eig = nx.eigenvector_centrality(G_gc)

centrality_df = pd.DataFrame({
    "stop_id": list(G_gc.nodes()),
    "betweenness": [bet[n] for n in G_gc.nodes()],
    "closeness": [close[n] for n in G_gc.nodes()],
    "eigenvector": [eig[n] for n in G_gc.nodes()],
})

# Attach stop names
centrality_df = centrality_df.merge(
    u_stops[["stop_id", "stop_name"]],
    on="stop_id",
    how="left"
)

# Top 10 by betweenness
display(
    centrality_df.sort_values("betweenness", ascending=False)
    .head(10)
)

# --- Plot distributions ---
plt.figure(figsize=(6, 4))
plt.hist(centrality_df["betweenness"], bins=30)
plt.xlabel("Betweenness centrality")
plt.ylabel("Number of stations")
plt.title("Betweenness centrality distribution (Berlin U-Bahn)")
plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.hist(centrality_df["closeness"], bins=30)
plt.xlabel("Closeness centrality")
plt.ylabel("Number of stations")
plt.title("Closeness centrality distribution (Berlin U-Bahn)")
plt.tight_layout()
plt.show()

# --- Helper: average shortest path on giant component ---
def giant_component_asp(H):
    if H.number_of_nodes() == 0:
        return np.nan
    if not nx.is_connected(H):
        comp = max(nx.connected_components(H), key=len)
        H = H.subgraph(comp)
    return nx.average_shortest_path_length(H)

# --- 1) Baseline on current giant component ---
baseline_asp = giant_component_asp(G_gc)
print("Baseline ASP (giant comp):", baseline_asp)

# --- 2) Find top 4 hubs by degree ---
deg = dict(G_gc.degree())
top_hubs = sorted(deg, key=deg.get, reverse=True)[:4]

# Map stop_id -> stop_name for nicer printing
id2name = dict(zip(u_stops["stop_id"], u_stops["stop_name"]))

print("\nTop 4 hubs (by degree):")
for i, h in enumerate(top_hubs, start=1):
    print(f"{i}. {h}  ({id2name.get(h, 'unknown name')})")

# --- 3) Remove hubs one by one and recompute ASP ---
print("\nStep-by-step effect of cumulatively removing hubs:")
print(f"Step 0 (no hubs removed): ASP = {baseline_asp:.3f}")

H_step = G_gc.copy()
asp_per_step = [baseline_asp]

for i, hub in enumerate(top_hubs, start=1):
    H_step.remove_node(hub)
    asp_i = giant_component_asp(H_step)
    asp_per_step.append(asp_i)
    print(f"Step {i} (removed hub {hub} – {id2name.get(hub, 'unknown')}): "
          f"ASP = {asp_i:.3f}")

# --- 4) Plot ASP vs number of hubs removed ---
steps = list(range(0, len(top_hubs) + 1))

plt.figure(figsize=(6, 4))
plt.plot(steps, asp_per_step, marker="o")
plt.xticks(steps)
plt.xlabel("Number of top hubs removed (cumulative)")
plt.ylabel("Average shortest path length (giant comp)")
plt.title("Step-by-step impact of removing top 4 hubs")
plt.tight_layout()
plt.show()

# --- All-pairs shortest path lengths on giant component ---
# This returns a dict: source -> {target: distance_in_hops}
spl_dict = dict(nx.all_pairs_shortest_path_length(G_gc))

# Choose time threshold (in minutes) and time per hop
time_per_hop = 2.0  # minutes per edge
T = 20.0            # total travel time threshold in minutes

accessibility = {}  # stop_id -> number of stations reachable within T

for u in G_gc.nodes():
    lengths = spl_dict[u]
    cnt = 0
    for v, d in lengths.items():
        travel_time = d * time_per_hop
        if travel_time <= T:
            cnt += 1
    accessibility[u] = cnt

acc_df = pd.DataFrame({
    "stop_id": list(accessibility.keys()),
    "accessibility_T{}".format(int(T)): list(accessibility.values())
})

acc_df = acc_df.merge(
    u_stops[["stop_id", "stop_name", "stop_lat", "stop_lon"]],
    on="stop_id",
    how="left"
)

display(acc_df.head())

# --- Histogram of accessibility ---
plt.figure(figsize=(6, 4))
plt.hist(acc_df["accessibility_T{}".format(int(T))], bins=20)
plt.xlabel(f"Number of stations reachable ≤ {int(T)} min")
plt.ylabel("Number of stations")
plt.title(f"Accessibility distribution (T = {int(T)} min, Berlin U-Bahn)")
plt.tight_layout()
plt.show()

# Center map on U-Bahn stops
center_lat = acc_df["stop_lat"].mean()
center_lon = acc_df["stop_lon"].mean()

m_acc = folium.Map(location=[center_lat, center_lon], zoom_start=12)

acc_col = "accessibility_T{}".format(int(T))
acc_min = acc_df[acc_col].min()
acc_max = acc_df[acc_col].max()

# Simple linear scaling of marker radius
def scale_radius(val, vmin, vmax, rmin=3, rmax=12):
    if vmax == vmin:
        return (rmin + rmax) / 2.0
    return rmin + (rmax - rmin) * (val - vmin) / (vmax - vmin)

for _, row in acc_df.iterrows():
    radius = scale_radius(row[acc_col], acc_min, acc_max)
    folium.CircleMarker(
        location=[row["stop_lat"], row["stop_lon"]],
        radius=radius,
        popup=f"{row['stop_name']} (acc={row[acc_col]})",
        fill=True,
        fill_opacity=0.7,
    ).add_to(m_acc)

m_acc